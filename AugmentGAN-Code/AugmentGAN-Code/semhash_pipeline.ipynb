{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking using SemHash on NLU Evaluation Corpora\n",
    "\n",
    "This notebook benchmarks the results on the 3 NLU Evaluation Corpora:\n",
    "1. Ask Ubuntu Corpus\n",
    "2. Chatbot Corpus\n",
    "3. Web Application Corpus\n",
    "\n",
    "\n",
    "More information about the dataset is available here: \n",
    "\n",
    "https://github.com/sebischair/NLU-Evaluation-Corpora\n",
    "\n",
    "\n",
    "* Semantic Hashing is used as a featurizer. The idea is taken from the paper:\n",
    "\n",
    "https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/\n",
    "\n",
    "* Benchmarks are performed on the same train and test datasets used by the other benchmarks performed in the past. One important paper that benchmarks the datasets mentioned above on some important platforms (Dialogflow, Luis, Watson and RASA) is : \n",
    "\n",
    "http://workshop.colips.org/wochat/@sigdial2017/documents/SIGDIAL22.pdf\n",
    "\n",
    "* Furthermore, Botfuel made another benchmarks with more platforms (Recast, Snips and their own) and results can be found here: \n",
    "\n",
    "https://github.com/Botfuel/benchmark-nlp-2018\n",
    "\n",
    "* The blogposts about the benchmarks done in the past are available at : \n",
    "\n",
    "https://medium.com/botfuel/benchmarking-intent-classification-services-june-2018-eb8684a1e55f\n",
    "\n",
    "https://medium.com/snips-ai/an-introduction-to-snips-nlu-the-open-source-library-behind-snips-embedded-voice-platform-b12b1a60a41a\n",
    "\n",
    "* To be very fair on our benchmarks and results, we used the same train and test set used by the other benchmarks and no cross validation or stratified splits were used. The test data was not used in any way to improve the results. The dataset used can be found here:\n",
    "\n",
    "https://github.com/Botfuel/benchmark-nlp-2018/tree/master/results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import csv\n",
    "import spacy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy english dataset needs to be present. It can be downloaded using the following command:\n",
    "\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_dict_AskUbuntu = {\"Make Update\":0, \"Setup Printer\":1, \"Shutdown Computer\":2, \"Software Recommendation\":3, \"None\":4}\n",
    "intent_dict_Chatbot = {\"DepartureTime\":0, \"FindConnection\":1}\n",
    "intent_dict_WebApplications = {\"Download Video\":0, \"Change Password\":1, \"None\":2, \"Export Data\":3, \"Sync Accounts\":4,\n",
    "                  \"Filter Spam\":5, \"Find Alternative\":6, \"Delete Account\":7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_dataset = 'WebApplications' #choose from 'AskUbuntu', 'Chatbot' or 'WebApplications'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_CSV_datafile(filename):    \n",
    "    X = []\n",
    "    y = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            X.append(row[0])\n",
    "            if benchmark_dataset == 'AskUbuntu':\n",
    "                y.append(intent_dict_AskUbuntu[row[1]])\n",
    "            elif benchmark_dataset == 'Chatbot':\n",
    "                y.append(intent_dict_Chatbot[row[1]])\n",
    "            else:\n",
    "                y.append(intent_dict_WebApplications[row[1]])           \n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train = \"./datasets/KL/WebApplication/train.csv\"\n",
    "filename_test= \"./datasets/KL/WebApplication/test.csv\"\n",
    "X_train_raw, y_train_raw = read_CSV_datafile(filename = filename_train)\n",
    "X_test_raw, y_test_raw = read_CSV_datafile(filename = filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data samples: \n",
      " ['How do I delete my Gmail account?', 'How do I delete my Experts Exchange account?', 'How do I delete my Ohloh profile?', 'How can I permanently delete my MySpace account?'] \n",
      "\n",
      "\n",
      "Class Labels: \n",
      " [7, 7, 7, 7] \n",
      "\n",
      "\n",
      "Size of Training Data: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data samples: \\n\",X_train_raw[-5:-1], \"\\n\\n\")\n",
    "\n",
    "print(\"Class Labels: \\n\", y_train_raw[-5:-1], \"\\n\\n\")\n",
    "\n",
    "print(\"Size of Training Data: {}\".format(len(X_train_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    \"\"\"\n",
    "    Returns a list of strings containing each token in `sentence`\n",
    "    \"\"\"\n",
    "    #return [i for i in re.split(r\"([-.\\\"',:? !\\$#@~()*&\\^%;\\[\\]/\\\\\\+<>\\n=])\",\n",
    "    #                            doc) if i != '' and i != ' ' and i != '\\n']\n",
    "    tokens = []\n",
    "    doc = nlp.tokenizer(doc)\n",
    "    for token in doc:\n",
    "        tokens.append(token.text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    clean_tokens = []\n",
    "    doc = nlp(doc)\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            clean_tokens.append(token.lemma_)\n",
    "    return \" \".join(clean_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "def semhash_tokenizer(text):\n",
    "    tokens = text.split(\" \")\n",
    "    final_tokens = []\n",
    "    for unhashed_token in tokens:\n",
    "        hashed_token = \"#{}#\".format(unhashed_token)\n",
    "        final_tokens += [''.join(gram)\n",
    "                         for gram in list(find_ngrams(list(hashed_token), 3))]\n",
    "    return final_tokens\n",
    "\n",
    "def semhash_corpus(corpus):\n",
    "    new_corpus = []\n",
    "    for sentence in corpus:\n",
    "        sentence = preprocess(sentence)\n",
    "        tokens = semhash_tokenizer(sentence)\n",
    "        new_corpus.append(\" \".join(map(str,tokens)))\n",
    "    return new_corpus\n",
    "\n",
    "X_train_raw = semhash_corpus(X_train_raw)\n",
    "X_test_raw = semhash_corpus(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(corpus, preprocessor=None, tokenizer=None):\n",
    "    vectorizer = CountVectorizer(ngram_range=(2,4),analyzer='char')\n",
    "    vectorizer.fit(corpus)\n",
    "    return vectorizer, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(clf, X_train, y_train, X_test, y_test, target_names,\n",
    "              print_report=True, feature_names=None, print_top10=False,\n",
    "              print_cm=True):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    #print(\"Accuracy: %0.3f (+/- %0.3f)\" % (score.mean(), score.std() * 2))\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if print_top10 and feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate([\"Make Update\", \"Setup Printer\", \"Shutdown Computer\",\"Software Recommendation\", \"None\"]):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join([feature_names[i] for i in top10]))))\n",
    "        print()\n",
    "\n",
    "    if print_report:\n",
    "        print(\"classification report:\")\n",
    "        print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=target_names))\n",
    "\n",
    "    if print_cm:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    # make some plots\n",
    "    indices = np.arange(len(results))\n",
    "\n",
    "    results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "    clf_names, score, training_time, test_time = results\n",
    "    training_time = np.array(training_time) / np.max(training_time)\n",
    "    test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Score\")\n",
    "    plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "    plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "             color='c')\n",
    "    plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "    plt.yticks(())\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplots_adjust(left=.25)\n",
    "    plt.subplots_adjust(top=.95)\n",
    "    plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "    for i, c in zip(indices, clf_names):\n",
    "        plt.text(-.3, i, c)\n",
    "    plt.savefig(\"./results_AskUbuntu.png\", format=\"png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_training():\n",
    "    vectorizer, feature_names = get_vectorizer(X_train_raw, preprocessor=preprocess, tokenizer=tokenize)\n",
    "    \n",
    "    X_train_no_HD = vectorizer.transform(X_train_raw).toarray()\n",
    "    X_test_no_HD = vectorizer.transform(X_test_raw).toarray()\n",
    "            \n",
    "    return X_train_no_HD, y_train_raw, X_test_no_HD, y_test_raw, feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, feature_names = data_for_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     X_train, y_train, X_test, y_test, feature_names = data_for_training()\n",
    "def ngram_encode(str_test, HD_aphabet, aphabet, n_size): # method for mapping n-gram statistics of a word to an N-dimensional HD vector\n",
    "    HD_ngram = np.zeros(HD_aphabet.shape[1]) # will store n-gram statistics mapped to HD vector\n",
    "    full_str = '#' + str_test + '#' # include extra symbols to the string\n",
    "        \n",
    "    for il, l in enumerate(full_str[:-(n_size-1)]): # loops through all n-grams\n",
    "        hdgram = HD_aphabet[aphabet.find(full_str[il]), :] # picks HD vector for the first symbol in the current n-gram\n",
    "        for ng in range(1, n_size): #loops through the rest of symbols in the current n-gram\n",
    "            hdgram = hdgram * np.roll(HD_aphabet[aphabet.find(full_str[il+ng]), :], ng) # two operations simultaneously; binding via elementvise multiplication; rotation via cyclic shift\n",
    "            \n",
    "        HD_ngram += hdgram # increments HD vector of n-gram statistics with the HD vector for the currently observed n-gram\n",
    "    \n",
    "    HD_ngram_norm = np.sqrt(HD_aphabet.shape[1]) * (HD_ngram/ np.linalg.norm(HD_ngram) )  # normalizes HD-vector so that its norm equals sqrt(N)       \n",
    "    return HD_ngram_norm # output normalized HD mapping\n",
    "\n",
    "\n",
    "\n",
    "N = 1000 # set the desired dimensionality of HD vectors\n",
    "n_size=3 # n-gram size\n",
    "aphabet = 'abcdefghijklmnopqrstuvwxyz#' #fix the alphabet. Note, we assume that capital letters are not in use \n",
    "np.random.seed(1) # for reproducibility\n",
    "HD_aphabet = 2 * (np.random.randn(len(aphabet), N) < 0) - 1 # generates bipolar {-1, +1}^N HD vectors; one random HD vector per symbol in the alphabet\n",
    "\n",
    "# str='High like a basketball jump' # example string to represent using n-grams\n",
    "\n",
    "# print(len(ngram_encode(str, HD_aphabet, aphabet, n_size))) # HD_ngram is a projection of n-gram statistics for str to N-dimensional space. It can be used to learn the word embedding\n",
    "# print(X_train_raw[:1])\n",
    "\n",
    "for i in range(len(X_train_raw)):\n",
    "     X_train_raw[i] = ngram_encode(X_train_raw[i], HD_aphabet, aphabet, n_size) # HD_ngram is a projection of n-gram statistics for str to N-dimensional space. It can be used to learn the word embedding\n",
    "# print(X_train_raw[:5])\n",
    "for i in range(len(X_test_raw)):\n",
    "    X_test_raw[i] = ngram_encode(X_test_raw[i], HD_aphabet, aphabet, n_size)\n",
    "# print(X_test_raw[:5])\n",
    "\n",
    "X_train, y_train, X_test, y_test = X_train_raw, y_train_raw, X_test_raw, y_test_raw\n",
    "# print(X_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Download Video': 1, 'Change Password': 2, 'None': 2, 'Export Data': 2, 'Sync Accounts': 3, 'Filter Spam': 6, 'Find Alternative': 7, 'Delete Account': 7}\n"
     ]
    }
   ],
   "source": [
    "count = {}\n",
    "for i in range(len(y_train)):\n",
    "    try:\n",
    "        count[target_names[y_train[i]]]+=1\n",
    "    except:\n",
    "        count[target_names[y_train[i]]] = 1\n",
    "print(str(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = list(intent_dict_WebApplications.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Download Video',\n",
       " 'Change Password',\n",
       " 'None',\n",
       " 'Export Data',\n",
       " 'Sync Accounts',\n",
       " 'Filter Spam',\n",
       " 'Find Alternative',\n",
       " 'Delete Account']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Split 0\n",
      "Train Size: 30\n",
      "Test Size: 59\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier(C=100, average=False, class_weight=None,\n",
      "                            early_stopping=False, fit_intercept=True,\n",
      "                            loss='hinge', max_iter=1000, n_iter_no_change=5,\n",
      "                            n_jobs=None, random_state=None, shuffle=True,\n",
      "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
      "                            warm_start=False)\n",
      "train time: 0.014s\n",
      "test time:  0.024s\n",
      "accuracy:   0.797\n",
      "dimensionality: 1000\n",
      "density: 1.000000\n",
      "\n",
      "classification report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  Download Video       0.00      0.00      0.00         0\n",
      " Change Password       1.00      0.67      0.80         6\n",
      "            None       0.00      0.00      0.00         4\n",
      "     Export Data       1.00      0.33      0.50         3\n",
      "   Sync Accounts       1.00      0.83      0.91         6\n",
      "     Filter Spam       0.76      0.93      0.84        14\n",
      "Find Alternative       0.75      0.94      0.83        16\n",
      "  Delete Account       0.82      0.90      0.86        10\n",
      "\n",
      "        accuracy                           0.80        59\n",
      "       macro avg       0.67      0.57      0.59        59\n",
      "    weighted avg       0.78      0.80      0.77        59\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  0  0  0  0  0  0  0]\n",
      " [ 0  4  0  0  0  1  0  1]\n",
      " [ 1  0  0  0  0  2  1  0]\n",
      " [ 0  0  0  1  0  1  1  0]\n",
      " [ 0  0  0  0  5  0  1  0]\n",
      " [ 0  0  0  0  0 13  1  0]\n",
      " [ 0  0  0  0  0  0 15  1]\n",
      " [ 0  0  0  0  0  0  1  9]]\n",
      "\n",
      "Random forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "train time: 0.056s\n",
      "test time:  0.003s\n",
      "accuracy:   0.678\n",
      "classification report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj18025/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/suraj18025/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-7795d426bb27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         results.append(benchmark(clf, X_train, y_train, X_test, y_test, target_names,\n\u001b[0;32m---> 36\u001b[0;31m                                  feature_names=feature_names))\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;31m#        # print('parameters')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#        # print(clf.grid_scores_[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-e2873bd49c4c>\u001b[0m in \u001b[0;36mbenchmark\u001b[0;34m(clf, X_train, y_train, X_test, y_test, target_names, print_report, feature_names, print_top10, print_cm)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"classification report:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         print(metrics.classification_report(y_test, pred,\n\u001b[0;32m---> 42\u001b[0;31m                                             target_names=target_names))\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprint_cm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1877\u001b[0m             )\n\u001b[1;32m   1878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 7, does not match size of target_names, 8. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i_s, split in enumerate(range(1)):\n",
    "    print(\"Evaluating Split {}\".format(i_s))\n",
    "#     X_train, y_train, X_test, y_test, feature_names = data_for_training()\n",
    "    #target_names = [\"Make Update\", \"Setup Printer\", \"Shutdown Computer\",\"Software Recommendation\", \"None\"]\n",
    "    print(\"Train Size: {}\\nTest Size: {}\".format(X_train.shape[0], X_test.shape[0]))\n",
    "    results = []\n",
    "    #alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "    parameters_mlp={'hidden_layer_sizes':[(100,50), (300, 100),(300,200,100)]}\n",
    "    parameters_RF={ \"n_estimators\" : [50,60,70],\n",
    "           \"min_samples_leaf\" : [1, 11]}\n",
    "    k_range = list(range(3,7))\n",
    "    parameters_knn = {'n_neighbors':k_range}\n",
    "    knn=KNeighborsClassifier(n_neighbors=5)\n",
    "    for clf, name in [  \n",
    "#             (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "#             (GridSearchCV(knn,parameters_knn, cv=5),\"gridsearchknn\"),\n",
    "#             #(Perceptron(n_iter=50), \"Perceptron\"),\n",
    "#             (GridSearchCV(MLPClassifier(activation='tanh'),parameters_mlp, cv=5),\"gridsearchmlp\"),\n",
    "#            # (MLPClassifier(hidden_layer_sizes=(100, 50), activation=\"logistic\", max_iter=300), \"MLP\"),\n",
    "#             #(MLPClassifier(hidden_layer_sizes=(300, 100, 50), activation=\"logistic\", max_iter=500), \"MLP\"),\n",
    "#            # (MLPClassifier(hidden_layer_sizes=(300, 100, 50), activation=\"tanh\", max_iter=500), \"MLP\"),\n",
    "            (PassiveAggressiveClassifier(100), \"Passive-Aggressive\"),\n",
    "#            # (KNeighborsClassifier(n_neighbors=1), \"kNN\"),\n",
    "#            # (KNeighborsClassifier(n_neighbors=3), \"kNN\"),\n",
    "#            # (KNeighborsClassifier(n_neighbors=5), \"kNN\"),\n",
    "#             #(KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "#             (GridSearchCV(RandomForestClassifier(n_estimators=10),parameters_RF, cv=5),\"gridsearchRF\")\n",
    "#             #(RandomForestClassifier(n_estimators=10), \"Random forest\"),\n",
    "            (RandomForestClassifier(n_estimators=50), \"Random forest\")\n",
    "    ]:\n",
    "           \n",
    "#         print('=' * 80)\n",
    "        print(name)\n",
    "        results.append(benchmark(clf, X_train, y_train, X_test, y_test, target_names,\n",
    "                                 feature_names=feature_names))\n",
    "\n",
    "    print(\"LinearSVC with L1-based feature selection\")\n",
    "    plot_results(results)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as timmer\n",
    "\n",
    "def ngram_encode_mod(str_test, HD_aphabet, aphabet, n_size): # method for mapping n-gram statistics of a word to an N-dimensional HD vector\n",
    "    HD_ngram = np.zeros(HD_aphabet.shape[1]) # will store n-gram statistics mapped to HD vector\n",
    "    full_str = '#' + str_test + '#' # include extra symbols to the string\n",
    "    shift=n_size-1\n",
    "        \n",
    "    # form the vector for the first n-gram\n",
    "    hdgram = HD_aphabet[aphabet.find(full_str[0]), :] # picks HD vector for the first symbol in the current n-gram\n",
    "#     for ng in range(1, n_size): #loops through the rest of symbols in the current n-gram\n",
    "#         hdgram = hdgram * np.roll(HD_aphabet[aphabet.find(full_str[0+ng]), :], ng) # two operations simultaneously; binding via elementvise multiplication; rotation via cyclic shift        \n",
    "#     HD_ngram += hdgram # increments HD vector of n-gram statistics with the HD vector for the currently observed n-gram\n",
    "        \n",
    "#     for l in range(1, len(full_str)-shift):  # form all other n-grams using the HD vector for the first one      \n",
    "#         hdgram = np.roll( hdgram * HD_aphabet[aphabet.find(full_str[l-1]), :], -1) * np.roll(HD_aphabet[aphabet.find(full_str[l+shift]), :], shift) # improved implementation of forming HD vectors for n-grams\n",
    "#         HD_ngram += hdgram # increments HD vector of n-gram statistics with the HD vector for the currently observed n-gram\n",
    "    \n",
    "#     HD_ngram_norm = np.sqrt(HD_aphabet.shape[1]) * (HD_ngram/ np.linalg.norm(HD_ngram) )  # normalizes HD-vector so that its norm equals sqrt(N)       \n",
    "    \n",
    "    HD_ngram_tot = np.zeros(HD_aphabet.shape[1])\n",
    "    for n_size in range(2, 7):\n",
    "        HD_ngram_tot += ngram_encode_hybrid(str_test, HD_aphabet, aphabet, n_size)\n",
    "    HD_ngram_tot = np.sqrt(HD_aphabet.shape[1]) * (HD_ngram_tot/ np.linalg.norm(HD_ngram_tot))\n",
    "    \n",
    "#     return HD_ngram_norm # output normalized HD mapping\n",
    "    return HD_ngram_tot\n",
    "\n",
    "\n",
    "def ngram_encode_hybrid(str_test, HD_aphabet, aphabet, n_size): # method for mapping n-gram statistics of a word to an N-dimensional HD vector\n",
    "    HD_ngram = np.zeros(HD_aphabet.shape[1]) # will store n-gram statistics mapped to HD vector\n",
    "    full_str = '#' + str_test + '#' # include extra symbols to the string\n",
    "    shift=n_size-1\n",
    "#     print(full_str)\n",
    "    \n",
    "    if n_size < 3: # use the initial implementation\n",
    "        #adjust the string for n-gram size\n",
    "        if n_size == 1:\n",
    "            full_str_e=full_str            \n",
    "        else:\n",
    "            full_str_e=full_str[:-(n_size-1)]    \n",
    "            \n",
    "        for il, l in enumerate(full_str_e): # loops through all n-grams\n",
    "            hdgram = HD_aphabet[aphabet.find(full_str[il]), :] # picks HD vector for the first symbol in the current n-gram\n",
    "            \n",
    "            for ng in range(1, n_size): #loops through the rest of symbols in the current n-gram\n",
    "                    hdgram = hdgram * np.roll(HD_aphabet[aphabet.find(full_str[il+ng]), :], ng) # two operations simultaneously; binding via elementvise multiplication; rotation via cyclic shift\n",
    "    \n",
    "            HD_ngram += hdgram # increments HD vector of n-gram statistics with the HD vector for the currently observed n-gram\n",
    "\n",
    "    else:    # use the modified implementation\n",
    "\n",
    "        # form the vector for the first n-gram\n",
    "        hdgram = HD_aphabet[aphabet.find(full_str[0]), :] # picks HD vector for the first symbol in the current n-gram\n",
    "        for ng in range(1, n_size): #loops through the rest of symbols in the current n-gram\n",
    "            hdgram = hdgram * np.roll(HD_aphabet[aphabet.find(full_str[0+ng]), :], ng) # two operations simultaneously; binding via elementvise multiplication; rotation via cyclic shift        \n",
    "        HD_ngram += hdgram # increments HD vector of n-gram statistics with the HD vector for the currently observed n-gram\n",
    "            \n",
    "        for l in range(1, len(full_str)-shift): # form all other n-grams using the HD vector for the first one    \n",
    "            hdgram = np.roll( hdgram * HD_aphabet[aphabet.find(full_str[l-1]), :], -1) * np.roll(HD_aphabet[aphabet.find(full_str[l+shift]), :], shift) # improved implementation of forming HD vectors for n-grams\n",
    "            HD_ngram += hdgram # increments HD vector of n-gram statistics with the HD vector for the currently observed n-gram\n",
    "    \n",
    "    HD_ngram_norm = np.sqrt(HD_aphabet.shape[1]) * (HD_ngram/ np.linalg.norm(HD_ngram) )  # normalizes HD-vector so that its norm equals sqrt(N)    \n",
    "    \n",
    "    return HD_ngram_norm # output normalized HD mapping\n",
    "\n",
    "\n",
    "\n",
    "N_mod = 17000 # set the desired dimensionality of HD vectors\n",
    "n_size_mod=3 # n-gram size\n",
    "aphabet_mod = 'abcdefghijklmnopqrstuvwxyz# ?-/!\\+[]{}().,:;1234567890äöÿã' #fix the alphabet. Note, we assume that capital letters are not in use \n",
    "# np.random.seed(1) # for reproducibility\n",
    "HD_aphabet_mod = 2 * (np.random.randn(len(aphabet_mod), N_mod) < 0) - 1 # generates bipolar {-1, +1}^N HD vectors; one random HD vector per symbol in the alphabet\n",
    "\n",
    "#str='jump' # example string to represent using n-grams\n",
    "str_tst='intrigued started to work it out the third installment of the twilight saga eclipse proved to be a small stepup from the first two movies lowrie atoned for an error he made in the top half of the inning when he dropped a foul popup while playing first base but she couldnt sleep knowing that her mother was sitting alone somewhere in a big foreign airport that had gone into crisis mode call the minot at to reserve your spot for beginning duplicate bridge monday evenings plucking the track from her am sasha fierce beyonces halo has become one of her signature songs wow sound smart should change his name to really dumb it dabbles in several facets of the financial services industry from its jpmorgan investment banking chase credit cards and various banking operations the developing industrial applications for silver are exciting and expect longterm growth here rick morgan has been fishing lower manitou since he was a kid growing up in hibbing in the late he may not be a good guy but hes not a rapist at least not yet we have taken this request under consideration in gardner was facing trial for killing melvyn otterstrom a bartender in salt lake city when a girlfriend slipped him a gun at the courthouse soon after more offers came piling in from other schools central michigan miami western michigan and ohio for the current crop of condo dwellers losing the convenience of a car is part of the price of home ownership by values were in retreat will julian crocker look into this small problem but she said that officer didnt want to hear any more these were on increases in gross and average from their pathetic predecessors though neither measure was ever an accurate barometer of industry health the participants bacon cheeseburger patty with bacon lettuce tomato and mayo phillies catcher carlos ruiz connects for a gamewinning home run in the inning against the cardinals tango don cheadle is an undercover cop posing as a drug lord watching his criminal compatriot wesley snipes in a restrained return to form return to power after a lengthy prison stretch we are eliminating sales and management company cars altogether and moving those positions to an allowance said lisa kneggs fleet manager on a per school dayhourly basis has anyone done the math on this why dont we just pay everyone not to work peacock asks referring to losing the streak lot depends on the pitching staff of course but as the smoke cleared lost became as twitter fan joe hewitt put it a soap opera not an intellectual scifi thriller and thus a waste of time that is something hill hopes they all take seriously in worlds recent tests on mobile bandwidth speeds ranks first hands down its too early to judge whether a large number of donors across the country will pour money into the race as they did for brown but browns appearance here is likely to increase that possibility as the contest gains notice president ahmadinejad further announced irans decision to postpone talks with the world powers until august in a move to punish the west weir fire escape extension burley bridge mills viaduct road burley hes a house republican guy he may have been wearing number on his hip but tonights reebok boston indoor games at the reggie lewis center here was anything but unlucky for twotime olympic medallist bernard lagat adam vinatieri answered with a bomb of his own from yards many towns around the state are facing tightening budgets as certain services and repairs are left for another day whatrsquos amazing about jayz is that even nearing an age unheard of for a viable rapper even a decade ago the artist keeps putting out genredefying and essential albums is it rugged enough to handle outside terrain the war is coming to their cities the chechen rebel leader said in an interview on an islamist website he lasted only innings for his secondshortest outing of ford motor co has extended buyout offers once again to workers at its hamburg stamping plant a sign that the automaker is pushing hard to reduce its production work force rich barber says thats as it should be card companies can slap a penalty interest rate on existing balances if a customer falls at least days behind on payments obama then took on jay leno the nights entertainment saying great to see you jay the order was made through ovhs automated systems on the eve of retirement minnesota duluth chancellor kathryn martin seeks to finish her transformation of campus you are now over limit and so an over limit fee of rs can also be applied making the owed balance on the first page youll get photos of white folks being beaten up by groups of blacks and arabs in the spring mcdaniels made brandon marshall and tony scheffler who werent shy about expressing criticism his new jay cutlers its a sad day when a collegeuniversity honours a politician whose claim to fame was the businessoriented common sense revolution over someone anyone whose life serveds to inspire students to celebrate education the change while not material was accounted for on a retrospective basis and more closely aligns the depreciation policies with those of the companys drilling rigs which are depreciated based on operating days lets go to henry its popup menu lists all the pcs in your house that have been prepared for remote controlling including the kitchen laptop but there were other places to take a dip and a handful of bajans were taking advantage as dozed horn already had worked out for teams such as the carolina panthers and the philadelphia eagles but his times were in the mediocre to range notre dame beat providence next at syracuse saturday evolution as a search engine for commensurate energy and nutrient niches was now possible evolution as a euros the survival of the most fitting on this or any other wet rocky and sunlit planet and in any galaxy taking legal and critical measures of control in the sphere on nuclear security the international community should not ignore the global trends in energy and high technologies the judges did not goof in picking the medal winners mostly because the athletes last night made their jobs so easy ryan cross or potentially digby iaoni seem logical to me to add a bit of size what are your thoughts susan joy share holds one of her fish puppets from the sleep of waters theatrical presentation at her animated library exhibit wednesday at out north the battery gives me about hours usage under normal web browsing together with some office applications it really pisses me off submitted by mikel on sat we should also stop taxing businesses as individuals but rather reduce rates to which would help business to grow and create jobs ousley also won titles in breakaway roping and bradley in tiedown roping beyond that im not sure anybody agrees on anything be ready for a bunch of hostiles they were kind of targeting him and if youre not giving your best effort it allows the other team to feed off of that troy orion tom of beclabito killed aug by an improvised explosives device when his squad went to the aid of another that had come under heavy fire how many did it during the era we had the answers designed for those who recently lost a loved one he would like to provide more coverage to the uninsured by allowing people to buy into medicaid coverage an unknown suspect shot into a the residence and a vehicle that was parked in front of craig st neither the vehicle nor the residence was occupied at the time backtoback home runs by dollar ridgell and jerry powell highlighted an eightrun firstinning as morehouse went on to a convincing second round victory ron mcclelland the pastpresident of the and law association sees the changes as a case of jobs and services being moved out of the community ryan wittman has the game is upsidedown ahh well the lower prices just give myself and you more time to get more gold before it moves much higher so take it as a blessing in disguise shes the best candidate weve had since shandy finnessey but better earth day events include an art exhibition featuring kimberly piazzas tire recycling concepts at the pacific pinball museum in alameda and a screening of natures half acre about insects and flowers at the walt disney family museum in san francisco most of the paratroopers are still arriving trying to assess conditions and find the right local officials to work with the most lush lowkey spot in the world with a code attached to it she really comes across as a hypocrite steve teques was fifth in the the anniversary earth day will be observed thursday and the mesa republic is taking a look at four local businesses that use green practices days a year harry and nancy plastered them up at every press conference health care is legislation and the supposed divide of polls are within the margin of error of said pollsin which case there is no provable divide and the use journalistically of the word divide is inappropriate they would show about as much understanding of our system reagan agreed immediately it is a known fact what we arent paying out in decent shelter and living allowances we are paying out later in medical expenses prisons and child protection services we enter like animals and go out like animals says samir sublaban a grocery store owner he has missed straight games since suffering a torn groin at ottawa on jan and isnt expected to return until after the olympic break theyre not viable in commercial theaters without movie stars at least the government will then give you a tax break so you can buy a house cousins is taller than jefferson but should be able to put up similar doubledouble numbers cant even seem to sign up on atts website to check that means the tests are set to ignore the majority of banks holdings of sovereign debt as they moved the riskier elements of the debt out of their trading books so that it was left out of the scope of the tests compelling story lines involving the city of new orleans and itsongoing recovery from hurricane katrina and the attempt at a secondsuper bowl ring for indianapolis quarterback peyton manningpropelled the viewership they lost their mental strength after qualifying for the world cup strike while the irons hot unite says another was not sent a form and while their lives dont revolve solely around potter these days its still a source of good fun lynn neuenswander public information officer for the department of behavioral health part of the countys continuum of care said the face of homelessness is changing so please leave well enough alone unless you can do better or help in some way he said once a month he plans to open both sides both party central and party zone for larger parties juan uribe walked and sanchez doubled home sandoval loan from a bank or credit union more financial institutions are offering shortterm loans to people with poor credit with the mailout of census questionnaires slightly more than a month away the census bureau will run three ads promoting census awareness during the super bowl telecast two during the pregame show and one during the third quarter she is saying she is doing this for fun and why cant moms have fun rookie percy harvin is a real threat as is visanthe shiancoe tds including playoffs how are we to judge the austerity measures just passed by the merkel government ms chithra would not be eliminated or replaced by anybody think ill miss the walks to the stadium more than the games themselves its dans understanding and we have neurological evidence that abbie is not capable of seeing or having any cognitive understanding of the children even if they were standing in front of her greene said predicts a percent increase in the number of marylanders traveling by car this weekend in trying to find his best diego led argentina into battle on occasions with a record of luftha'\n",
    "\n",
    "start = timmer.time()\n",
    "HD_ngram_mod = ngram_encode_mod(str_tst, HD_aphabet_mod, aphabet_mod, n_size_mod) # HD_ngram is a projection of n-gram statistics for str to N-dimensional space. It can be used to learn the word embedding\n",
    "end = timmer.time()\n",
    "print(end - start) # get execution time\n",
    "\n",
    "start = timmer.time()\n",
    "HD_ngram2_mod = ngram_encode_hybrid(str_tst, HD_aphabet_mod, aphabet_mod, n_size_mod) # HD_ngram is a projection of n-gram statistics for str to N-dimensional space. It can be used to learn the word embedding\n",
    "end = timmer.time()\n",
    "print(end - start)  # get execution time\n",
    "\n",
    "print(np.sum(np.abs(HD_ngram2_mod-HD_ngram_mod))) # check correctness of obtained HD vectors\n",
    "print(HD_ngram2_mod)\n",
    "print(HD_ngram_mod[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
